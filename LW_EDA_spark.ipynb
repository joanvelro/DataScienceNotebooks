{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LW_EDA_spark.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP9yDp9W5WB+wTH7rEiHnxg"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"jaMq_Onrjb3w","executionInfo":{"status":"ok","timestamp":1605001688646,"user_tz":-60,"elapsed":1671,"user":{"displayName":"JOSE ANGEL VELASCO RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3EC8_a0A6OeFZcWQQL9_y1cn6e_9LpWYmJXvm_Q=s64","userId":"03103114229375004476"}}},"source":["import pandas as pd\n","import os\n","\n","#os.environ['JAVA_HOME'] = \"C:\\Program Files\\Java\\jdk1.8.0_231\" # 'C:\\\\Program Files\\\\Java\\\\jre1.8.0_231'\n","#os.environ['SPARK_HOME']= 'D:\\\\spark-2.4.5-bin-hadoop2.7' #\"C:/Apache/spark-1.6.0\"\n","#os.environ['HADOOP_HOME']= 'D:\\\\hadoop-2.7.1' #\"C:/Apache/spark-1.6.0/winutils/\"\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"DDqI1qnej3x3","executionInfo":{"status":"ok","timestamp":1605001814241,"user_tz":-60,"elapsed":37868,"user":{"displayName":"JOSE ANGEL VELASCO RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3EC8_a0A6OeFZcWQQL9_y1cn6e_9LpWYmJXvm_Q=s64","userId":"03103114229375004476"}},"outputId":"55d1979c-327a-4a2f-8aee-4e0b6d8e7d1a","colab":{"base_uri":"https://localhost:8080/"}},"source":["%pip install pyspark"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n","\u001b[K     |████████████████████████████████| 204.2MB 62kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 46.0MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=9de2205bf164373806db25a51d090ac6f444f7f3b70d7d6578cd81258a4e5978\n","  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2elahQAsjofh","executionInfo":{"status":"ok","timestamp":1605001881714,"user_tz":-60,"elapsed":550,"user":{"displayName":"JOSE ANGEL VELASCO RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3EC8_a0A6OeFZcWQQL9_y1cn6e_9LpWYmJXvm_Q=s64","userId":"03103114229375004476"}},"outputId":"4ec6985d-c095-4b85-f0b6-bbbe3575766b","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Load pyspark\n","\n","def load_pyspark():\n","  \"\"\" This function load pyspark and show python version\n","  \"\"\"\n","  try:\n","      from pyspark.sql import SQLContext\n","      from pyspark import SparkContext\n","\n","      sc = SparkContext()\n","      sqlContext = SQLContext(sc)\n","\n","      print('Spark version:{}'.format(sc.version))\n","      print('python version:{}'.format(sc.pythonVer))\n","\n","  except Exception as exception_msg:\n","      print('(!) Error in load_pyspark: ' + str(exception_msg))\n","\n","# run \n","load_pyspark()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(!) Error in load_pyspark: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-7-1a36c38910b0>:10 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PB49rX13hnuK","executionInfo":{"status":"error","timestamp":1605001688907,"user_tz":-60,"elapsed":1924,"user":{"displayName":"JOSE ANGEL VELASCO RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3EC8_a0A6OeFZcWQQL9_y1cn6e_9LpWYmJXvm_Q=s64","userId":"03103114229375004476"}},"outputId":"cbca88d7-257c-4849-b9e1-9c52bb5a4e3c","colab":{"base_uri":"https://localhost:8080/","height":265}},"source":["path = '\\\\raw_data\\\\*.csv'\n","data_input = sqlContext.read.format(\"com.databricks.spark.csv\").options(header=True,\n","                                                                          inferSchema=True,\n","                                                                          sep=';',\n","                                                                          line_terminator='\\n').load(path)\n","  "],"execution_count":2,"outputs":[{"output_type":"stream","text":["(!) Error loading pyspark\n","Loading\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-4d4f98a451c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     data_input = sqlContext.read.format(\"com.databricks.spark.csv\").options(header=True,\n\u001b[0m\u001b[1;32m     22\u001b[0m                                                                             \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                                             \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'sqlContext' is not defined"]}]},{"cell_type":"code","metadata":{"id":"MAp-dk_ujhCV"},"source":["print(data_input.printSchema())\n","  data_input.registerTempTable(\"data\")\n","  df = sqlContext.table(\"data\")"],"execution_count":null,"outputs":[]}]}