{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/10 19:34:19 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /tmp\n",
      "16/05/10 19:34:22 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "!hdfs dfs -rm -r -f /datasets /tmp\n",
    "!rm -rf /tmp/hadoop_git_readme*\n",
    "!hdfs dfs -expunge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 42241163264 (39.34 GB)\r\n",
      "Present Capacity: 37536710656 (34.96 GB)\r\n",
      "DFS Remaining: 37346992128 (34.78 GB)\r\n",
      "DFS Used: 189718528 (180.93 MB)\r\n",
      "DFS Used%: 0.51%\r\n",
      "Under replicated blocks: 0\r\n",
      "Blocks with corrupt replicas: 0\r\n",
      "Missing blocks: 0\r\n",
      "\r\n",
      "-------------------------------------------------\r\n",
      "Live datanodes (1):\r\n",
      "\r\n",
      "Name: 127.0.0.1:50010 (localhost)\r\n",
      "Hostname: sparkbox\r\n",
      "Decommission Status : Normal\r\n",
      "Configured Capacity: 42241163264 (39.34 GB)\r\n",
      "DFS Used: 189718528 (180.93 MB)\r\n",
      "Non DFS Used: 4704452608 (4.38 GB)\r\n",
      "DFS Remaining: 37346992128 (34.78 GB)\r\n",
      "DFS Used%: 0.45%\r\n",
      "DFS Remaining%: 88.41%\r\n",
      "Configured Cache Capacity: 0 (0 B)\r\n",
      "Cache Used: 0 (0 B)\r\n",
      "Cache Remaining: 0 (0 B)\r\n",
      "Cache Used%: 100.00%\r\n",
      "Cache Remaining%: 0.00%\r\n",
      "Xceivers: 1\r\n",
      "Last contact: Tue May 10 19:34:23 UTC 2016\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - vagrant supergroup          0 2016-05-10 19:05 /spark\r\n",
      "drwxr-xr-x   - vagrant supergroup          0 2016-05-10 18:48 /user\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem               Size     Used  Available  Use%\r\n",
      "hdfs://localhost:9000  39.3 G  180.9 M     34.8 G    0%\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -df -h /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179.0 M  /spark\r\n",
      "473.4 K  /user\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du -h /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget -q http://www.gutenberg.org/cache/epub/100/pg100.txt \\\n",
    "    -O ../datasets/shakespeare_all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -put ../datasets/shakespeare_all.txt \\\n",
    "    /datasets/shakespeare_all.txt\n",
    "\n",
    "!hdfs dfs -put ../datasets/hadoop_git_readme.txt \\\n",
    "    /datasets/hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 vagrant supergroup       1365 2016-05-10 19:34 /datasets/hadoop_git_readme.txt\r\n",
      "-rw-r--r--   1 vagrant supergroup    5589889 2016-05-10 19:34 /datasets/shakespeare_all.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /datasets/hadoop_git_readme.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat \\\n",
    "    hdfs:///datasets/hadoop_git_readme.txt \\\n",
    "    file:///home/vagrant/datasets/hadoop_git_readme.txt | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -cp /datasets/hadoop_git_readme.txt \\\n",
    "    /datasets/copy_hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/10 19:35:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n",
      "Deleted /datasets/copy_hadoop_git_readme.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm /datasets/copy_hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/10 19:35:09 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -expunge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -get /datasets/hadoop_git_readme.txt \\\n",
    "    /tmp/hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ntry, of \r\n",
      "encryption software.  BEFORE using any encryption software, please \r\n",
      "check your country's laws, regulations and policies concerning the\r\n",
      "import, possession, or use, and re-export of encryption software, to \r\n",
      "see if this is permitted.  See <http://www.wassenaar.org/> for more\r\n",
      "information.\r\n",
      "\r\n",
      "The U.S. Government Department of Commerce, Bureau of Industry and\r\n",
      "Security (BIS), has classified this software as Export Commodity \r\n",
      "Control Number (ECCN) 5D002.C.1, which includes information security\r\n",
      "software using or performing cryptographic functions with asymmetric\r\n",
      "algorithms.  The form and manner of this Apache Software Foundation\r\n",
      "distribution makes it eligible for export under the License Exception\r\n",
      "ENC Technology Software Unrestricted (TSU) exception (see the BIS \r\n",
      "Export Administration Regulations, Section 740.13) for both object \r\n",
      "code and source code.\r\n",
      "\r\n",
      "The following provides more details on the included cryptographic\r\n",
      "software:\r\n",
      "  Hadoop Core uses the SSL libraries from the Jetty project written \r\n",
      "by mortbay.org."
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /datasets/hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Snakebite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from snakebite.client import Client\n",
    "client = Client(\"localhost\", 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blockSize': 134217728L,\n",
       " 'bytesPerChecksum': 512,\n",
       " 'checksumType': 2,\n",
       " 'encryptDataTransfer': False,\n",
       " 'fileBufferSize': 4096,\n",
       " 'replication': 1,\n",
       " 'trashInterval': 0L,\n",
       " 'writePacketSize': 65536}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.serverdefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets\n",
      "/spark\n",
      "/user\n"
     ]
    }
   ],
   "source": [
    "for x in client.ls(['/']):\n",
    "    print x['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capacity': 42241163264L,\n",
       " 'corrupt_blocks': 0L,\n",
       " 'filesystem': 'hdfs://localhost:9000',\n",
       " 'missing_blocks': 0L,\n",
       " 'remaining': 37341663232L,\n",
       " 'under_replicated': 0L,\n",
       " 'used': 195353480L}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'length': 5591254L, 'path': '/datasets'},\n",
       " {'length': 187698038L, 'path': '/spark'},\n",
       " {'length': 484810L, 'path': '/user'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(client.du([\"/\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note:\n",
    "# put command is not yet available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "for el in client.cat(['/datasets/hadoop_git_readme.txt']):\n",
    "    print el.next().count(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note:\n",
    "# copy command is not yet available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/datasets/shakespeare_all.txt', 'result': True}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete(['/datasets/shakespeare_all.txt']).next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': '',\n",
       " 'path': '/tmp/hadoop_git_readme_2.txt',\n",
       " 'result': True,\n",
       " 'source_path': '/datasets/hadoop_git_readme.txt'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(client\n",
    ".copyToLocal(['/datasets/hadoop_git_readme.txt'], \n",
    "             '/tmp/hadoop_git_readme_2.txt')\n",
    ".next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '/datasets_2', 'result': True}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(client.mkdir(['/datasets_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'path': '/datasets', 'result': True},\n",
       " {'path': '/datasets_2', 'result': True}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(client.delete(['/datasets*'], recurse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
