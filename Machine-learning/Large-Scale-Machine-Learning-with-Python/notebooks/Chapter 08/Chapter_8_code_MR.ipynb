{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first insert some data in the HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 vagrant supergroup       1365 2016-05-10 19:58 /datasets/hadoop_git_readme.txt\r\n",
      "-rw-r--r--   1 vagrant supergroup    5589889 2016-05-10 19:58 /datasets/shakespeare_all.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /datasets\n",
    "!wget -q http://www.gutenberg.org/cache/epub/100/pg100.txt \\\n",
    "    -O ../datasets/shakespeare_all.txt\n",
    "!hdfs dfs -put -f ../datasets/shakespeare_all.txt /datasets/shakespeare_all.txt\n",
    "!hdfs dfs -put -f ../datasets/hadoop_git_readme.txt /datasets/hadoop_git_readme.txt\n",
    "!hdfs dfs -ls /datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MR with Hadoop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('mapper_hadoop.py', 'w') as fh:\n",
    "    fh.write(\"\"\"#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    print \"chars\", len(line.rstrip('\\\\n'))\n",
    "    print \"words\", len(line.split())\n",
    "    print \"lines\", 1\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "with open('reducer_hadoop.py', 'w') as fh:\n",
    "    fh.write(\"\"\"#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "\n",
    "counts = {\"chars\": 0, \"words\":0, \"lines\":0}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    kv = line.rstrip().split()\n",
    "    counts[kv[0]] += int(kv[1])\n",
    "\n",
    "for k,v in counts.items():\n",
    "    print k, v\n",
    "    \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x *_hadoop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars 1335\r\n",
      "lines 31\r\n",
      "words 179\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../datasets/hadoop_git_readme.txt | ./mapper_hadoop.py | sort -k1,1 | ./reducer_hadoop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/05/10 19:58:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /tmp/mr.out\n",
      "packageJobJar: [/tmp/hadoop-unjar5384590696382062055/] [] /tmp/streamjob1965588122940844531.jar tmpDir=null\n",
      "16/05/10 19:58:50 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/10 19:58:51 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "16/05/10 19:58:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/05/10 19:58:51 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/05/10 19:58:52 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1462906052477_0019\n",
      "16/05/10 19:58:52 INFO impl.YarnClientImpl: Submitted application application_1462906052477_0019\n",
      "16/05/10 19:58:52 INFO mapreduce.Job: The url to track the job: http://sparkbox:8088/proxy/application_1462906052477_0019/\n",
      "16/05/10 19:58:52 INFO mapreduce.Job: Running job: job_1462906052477_0019\n",
      "16/05/10 19:58:58 INFO mapreduce.Job: Job job_1462906052477_0019 running in uber mode : false\n",
      "16/05/10 19:58:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/05/10 19:59:03 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/05/10 19:59:08 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/05/10 19:59:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/05/10 19:59:14 INFO mapreduce.Job: Job job_1462906052477_0019 completed successfully\n",
      "16/05/10 19:59:14 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1060\n",
      "\t\tFILE: Number of bytes written=332854\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2256\n",
      "\t\tHDFS: Number of bytes written=33\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6732\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3739\n",
      "\t\tTotal time spent by all map tasks (ms)=6732\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3739\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=6732\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3739\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6893568\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3828736\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31\n",
      "\t\tMap output records=93\n",
      "\t\tMap output bytes=868\n",
      "\t\tMap output materialized bytes=1066\n",
      "\t\tInput split bytes=208\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=23\n",
      "\t\tReduce shuffle bytes=1066\n",
      "\t\tReduce input records=93\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=186\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=78\n",
      "\t\tCPU time spent (ms)=1830\n",
      "\t\tPhysical memory (bytes) snapshot=699170816\n",
      "\t\tVirtual memory (bytes) snapshot=2495647744\n",
      "\t\tTotal committed heap usage (bytes)=512229376\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2048\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=33\n",
      "16/05/10 19:59:14 INFO streaming.StreamJob: Output directory: /tmp/mr.out\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /tmp\n",
    "!hdfs dfs -rm -f -r /tmp/mr.out\n",
    "\n",
    "!hadoop jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar \\\n",
    "-files mapper_hadoop.py,reducer_hadoop.py \\\n",
    "-mapper mapper_hadoop.py -reducer reducer_hadoop.py \\\n",
    "-input /datasets/hadoop_git_readme.txt -output /tmp/mr.out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 vagrant supergroup          0 2016-05-10 19:59 /tmp/mr.out/_SUCCESS\r\n",
      "-rw-r--r--   1 vagrant supergroup         33 2016-05-10 19:59 /tmp/mr.out/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/mr.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars 1335\t\r\n",
      "lines 31\t\r\n",
      "words 179\t\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /tmp/mr.out/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MR with Python MrJob library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"MrJob_job1.py\", \"w\") as fh:\n",
    "    fh.write(\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        yield \"chars\", len(line)\n",
    "        yield \"words\", len(line.split())\n",
    "        yield \"lines\", 1\n",
    "\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRWordFrequencyCount.run()    \n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\r\n",
      "Creating temp directory /tmp/MrJob_job1.vagrant.20160510.195920.590984\r\n",
      "Running step 1 of 1...\r\n",
      "Streaming final output from /tmp/MrJob_job1.vagrant.20160510.195920.590984/output...\r\n",
      "\"chars\"\t1335\r\n",
      "\"lines\"\t31\r\n",
      "\"words\"\t179\r\n",
      "Removing temp directory /tmp/MrJob_job1.vagrant.20160510.195920.590984...\r\n"
     ]
    }
   ],
   "source": [
    "!python MrJob_job1.py ../datasets/hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Looking for hadoop binary in /usr/local/hadoop/bin...\n",
      "Found hadoop binary: /usr/local/hadoop/bin/hadoop\n",
      "Creating temp directory /tmp/MrJob_job1.vagrant.20160510.195920.870616\n",
      "Using Hadoop version 2.6.4\n",
      "Copying local files to hdfs:///user/vagrant/tmp/mrjob/MrJob_job1.vagrant.20160510.195920.870616/files/...\n",
      "Looking for Hadoop streaming jar in /usr/local/hadoop...\n",
      "Found Hadoop streaming jar: /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar7634308048659876233/] [] /tmp/streamjob5879999650692493094.jar tmpDir=null\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Connecting to ResourceManager at /0.0.0.0:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1462906052477_0020\n",
      "  Submitted application application_1462906052477_0020\n",
      "  The url to track the job: http://sparkbox:8088/proxy/application_1462906052477_0020/\n",
      "  Running job: job_1462906052477_0020\n",
      "  Job job_1462906052477_0020 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1462906052477_0020 completed successfully\n",
      "  Output directory: hdfs:///user/vagrant/tmp/mrjob/MrJob_job1.vagrant.20160510.195920.870616/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2048\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=36\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1153\n",
      "\t\tFILE: Number of bytes written=337717\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2256\n",
      "\t\tHDFS: Number of bytes written=36\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=7394304\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=3846144\n",
      "\t\tTotal time spent by all map tasks (ms)=7221\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=7221\n",
      "\t\tTotal time spent by all reduce tasks (ms)=3756\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3756\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=7221\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=3756\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1830\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=66\n",
      "\t\tInput split bytes=208\n",
      "\t\tMap input records=31\n",
      "\t\tMap output bytes=961\n",
      "\t\tMap output materialized bytes=1159\n",
      "\t\tMap output records=93\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=726175744\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=93\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=1159\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=186\n",
      "\t\tTotal committed heap usage (bytes)=515899392\n",
      "\t\tVirtual memory (bytes) snapshot=2496479232\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/vagrant/tmp/mrjob/MrJob_job1.vagrant.20160510.195920.870616/output...\n",
      "\"chars\"\t1335\n",
      "\"lines\"\t31\n",
      "\"words\"\t179\n",
      "Removing HDFS temp directory hdfs:///user/vagrant/tmp/mrjob/MrJob_job1.vagrant.20160510.195920.870616...\n",
      "Removing temp directory /tmp/MrJob_job1.vagrant.20160510.195920.870616...\n"
     ]
    }
   ],
   "source": [
    "!python MrJob_job1.py -r hadoop hdfs:///datasets/hadoop_git_readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"MrJob_job2.py\", \"w\") as fh:\n",
    "    fh.write(\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(mapper=self.mapper_word_count_one_key,\n",
    "                   reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        yield (word, sum(counts))\n",
    "    \n",
    "    def mapper_word_count_one_key(self, word, counts):\n",
    "        # send all the tuples to same reducer\n",
    "        yield None, (counts, word)\n",
    "\n",
    "    def reducer_find_max_word(self, _, count_word_pairs):\n",
    "        # each item of word_count_pairs is a tuple (count, word),\n",
    "        yield max(count_word_pairs)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRMostUsedWord.run()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27801\t\"the\"\r\n"
     ]
    }
   ],
   "source": [
    "# This time is running on a big dataset\n",
    "!python MrJob_job2.py --quiet ../datasets/shakespeare_all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27801\t\"the\"\r\n"
     ]
    }
   ],
   "source": [
    "!python MrJob_job2.py -r hadoop --quiet hdfs:///datasets/shakespeare_all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
